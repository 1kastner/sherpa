{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SHERPA is a Python library for hyperparameter tuning of machine learning models.\n",
    "Copyright (C) 2018  Lars Hertel, Peter Sadowski, and Julian Collado.\n",
    "\n",
    "This file is part of SHERPA.\n",
    "\n",
    "SHERPA is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "SHERPA is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with SHERPA.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "import sherpa\n",
    "from sherpa.algorithms import Genetic\n",
    "import time\n",
    "from keras.datasets import mnist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [sherpa.Discrete('hidden_size', [16, 512]),\n",
    "              sherpa.Discrete('n_layers', [1, 10]),\n",
    "              sherpa.Choice('activation', [F.relu, F.tanh, F.sigmoid]),\n",
    "              sherpa.Continuous('lr',[1e-4,1e-2]),\n",
    "              sherpa.Continuous('dropout',[0.0,1.0])]\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm= Genetic(max_num_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,in_size,out_size,n_layers,hidden_size,act,dropout):\n",
    "        super(MLP,self).__init__()\n",
    "        self.n_layers=n_layers\n",
    "        self.act=act\n",
    "        for i in range(n_layers):\n",
    "            if i==0:\n",
    "                layer_in_size=in_size\n",
    "            else:\n",
    "                layer_in_size=hidden_size\n",
    "            if i==(n_layers-1):\n",
    "                layer_out_size=out_size\n",
    "            else:\n",
    "                layer_out_size=hidden_size\n",
    "            \n",
    "            setattr(self,'dense_{}'.format(i),nn.Linear(layer_in_size,layer_out_size))\n",
    "            \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=x\n",
    "        for i in range(self.n_layers):\n",
    "            if i==(self.n_layers-1):\n",
    "                out=getattr(self,'dense_{}'.format(i))(self.dropout(out))\n",
    "            else:\n",
    "                out=self.act(getattr(self,'dense_{}'.format(i))(self.dropout(out)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=15\n",
    "batch_size=64\n",
    "train_data = DataLoader(TensorDataset(torch.from_numpy(x_train),torch.from_numpy(y_train).type(torch.long)),batch_size=batch_size,drop_last=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "x_test_tensor=torch.from_numpy(x_test)\n",
    "y_test_tensor=torch.from_numpy(y_test).type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sherpa.core:\n",
      "-------------------------------------------------------\n",
      "SHERPA Dashboard running. Access via\n",
      "http://127.0.1.1:8880 if on a cluster or\n",
      "http://localhost:8880 if running locally.\n",
      "-------------------------------------------------------\n",
      "Process Process-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/afuster/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/afuster/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/afuster/anaconda3/lib/python3.6/site-packages/flask/app.py\", line 938, in run\n",
      "    cli.show_server_banner(self.env, self.debug, self.name, False)\n",
      "  File \"/home/afuster/anaconda3/lib/python3.6/site-packages/flask/cli.py\", line 629, in show_server_banner\n",
      "    click.echo(message)\n",
      "  File \"/home/afuster/anaconda3/lib/python3.6/site-packages/click/utils.py\", line 259, in echo\n",
      "    file.write(message)\n",
      "io.UnsupportedOperation: not writable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1:\t{'hidden_size': 460, 'n_layers': 1, 'activation': <function tanh at 0x7f1ffe86b840>, 'lr': 0.00021695391812050625, 'dropout': 0.39973204847122445}\n",
      "0.9197999835014343\n",
      "Trial 2:\t{'hidden_size': 115, 'n_layers': 6, 'activation': <function tanh at 0x7f1ffe86b840>, 'lr': 0.009140637249645395, 'dropout': 0.7670655675929657}\n",
      "0.06909999996423721\n",
      "Trial 3:\t{'hidden_size': 166, 'n_layers': 5, 'activation': <function tanh at 0x7f1ffe86b840>, 'lr': 0.00034159143102930386, 'dropout': 0.3487156777277136}\n",
      "0.9642000198364258\n",
      "Trial 4:\t{'hidden_size': 78, 'n_layers': 7, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.007737311617395133, 'dropout': 0.710135204033945}\n",
      "0.10279999673366547\n",
      "Trial 5:\t{'hidden_size': 429, 'n_layers': 3, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008554026409167272, 'dropout': 0.5679616916002217}\n",
      "0.9617999792098999\n",
      "Trial 6:\t{'hidden_size': 120, 'n_layers': 2, 'activation': <function relu at 0x7f1ffe869268>, 'lr': 0.00838299992450946, 'dropout': 0.3813819115298105}\n",
      "0.9599999785423279\n",
      "Trial 7:\t{'hidden_size': 405, 'n_layers': 7, 'activation': <function tanh at 0x7f1ffe86b840>, 'lr': 0.0057696762773641564, 'dropout': 0.27694250953343436}\n",
      "0.7983999848365784\n",
      "Trial 8:\t{'hidden_size': 264, 'n_layers': 3, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.4042930677405344}\n",
      "0.9710999727249146\n",
      "Trial 9:\t{'hidden_size': 153, 'n_layers': 1, 'activation': <function relu at 0x7f1ffe869268>, 'lr': 0.007809347852051222, 'dropout': 0.20892622075317202}\n",
      "0.8970999717712402\n",
      "Trial 10:\t{'hidden_size': 60, 'n_layers': 4, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009005266118322943, 'dropout': 0.13462578033866046}\n",
      "0.9700999855995178\n",
      "Trial 11:\t{'hidden_size': 60, 'n_layers': 4, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008119018567625506, 'dropout': 0.4042930677405344}\n",
      "0.9514999985694885\n",
      "Trial 12:\t{'hidden_size': 264, 'n_layers': 6, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009005266118322943, 'dropout': 0.13462578033866046}\n",
      "0.9628000259399414\n",
      "Trial 13:\t{'hidden_size': 60, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009005266118322943, 'dropout': 0.13462578033866046}\n",
      "0.9689000248908997\n",
      "Trial 14:\t{'hidden_size': 60, 'n_layers': 3, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009005266118322943, 'dropout': 0.13462578033866046}\n",
      "0.968999981880188\n",
      "Trial 15:\t{'hidden_size': 60, 'n_layers': 3, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009005266118322943, 'dropout': 0.2276419665470265}\n",
      "0.9686999917030334\n",
      "Trial 16:\t{'hidden_size': 60, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009005266118322943, 'dropout': 0.13462578033866046}\n",
      "0.9696999788284302\n",
      "Trial 17:\t{'hidden_size': 60, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009005266118322943, 'dropout': 0.9311040055799629}\n",
      "0.5817999839782715\n",
      "Trial 18:\t{'hidden_size': 60, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009005266118322943, 'dropout': 0.13462578033866046}\n",
      "0.9688000082969666\n",
      "Trial 19:\t{'hidden_size': 407, 'n_layers': 3, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.13462578033866046}\n",
      "0.9768000245094299\n",
      "Trial 20:\t{'hidden_size': 264, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.13462578033866046}\n",
      "0.9779000282287598\n",
      "Trial 21:\t{'hidden_size': 264, 'n_layers': 6, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.4042930677405344}\n",
      "0.9625999927520752\n",
      "Trial 22:\t{'hidden_size': 264, 'n_layers': 3, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008283454143537825, 'dropout': 0.13462578033866046}\n",
      "0.9800000190734863\n",
      "Trial 23:\t{'hidden_size': 264, 'n_layers': 3, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.4042930677405344}\n",
      "0.9739000201225281\n",
      "Trial 24:\t{'hidden_size': 264, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008283454143537825, 'dropout': 0.13462578033866046}\n",
      "0.9761999845504761\n",
      "Trial 25:\t{'hidden_size': 407, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008283454143537825, 'dropout': 0.23873876732826038}\n",
      "0.9803000092506409\n",
      "Trial 26:\t{'hidden_size': 264, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.4042930677405344}\n",
      "0.9699000120162964\n",
      "Trial 27:\t{'hidden_size': 264, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008283454143537825, 'dropout': 0.13462578033866046}\n",
      "0.9796000123023987\n",
      "Trial 28:\t{'hidden_size': 60, 'n_layers': 4, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009005266118322943, 'dropout': 0.23873876732826038}\n",
      "0.9642000198364258\n",
      "Trial 29:\t{'hidden_size': 60, 'n_layers': 3, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.003001906916515257, 'dropout': 0.4042930677405344}\n",
      "0.9567000269889832\n",
      "Trial 30:\t{'hidden_size': 264, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008283454143537825, 'dropout': 0.13462578033866046}\n",
      "0.9764999747276306\n",
      "Trial 31:\t{'hidden_size': 264, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.3137333252258978}\n",
      "0.9758999943733215\n",
      "Trial 32:\t{'hidden_size': 207, 'n_layers': 3, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.13462578033866046}\n",
      "0.9776999950408936\n",
      "Trial 33:\t{'hidden_size': 407, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008283454143537825, 'dropout': 0.23873876732826038}\n",
      "0.9789000153541565\n",
      "Trial 34:\t{'hidden_size': 407, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008283454143537825, 'dropout': 0.2511163133899563}\n",
      "0.9796000123023987\n",
      "Trial 35:\t{'hidden_size': 264, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.3137333252258978}\n",
      "0.9758999943733215\n",
      "Trial 36:\t{'hidden_size': 264, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.13462578033866046}\n",
      "0.9767000079154968\n",
      "Trial 37:\t{'hidden_size': 264, 'n_layers': 3, 'activation': <function tanh at 0x7f1ffe86b840>, 'lr': 0.009493107893987409, 'dropout': 0.13462578033866046}\n",
      "0.9402999877929688\n",
      "Trial 38:\t{'hidden_size': 407, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008283454143537825, 'dropout': 0.23873876732826038}\n",
      "0.9782999753952026\n",
      "Trial 39:\t{'hidden_size': 264, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.13462578033866046}\n",
      "0.979200005531311\n",
      "Trial 40:\t{'hidden_size': 264, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008283454143537825, 'dropout': 0.13462578033866046}\n",
      "0.9747999906539917\n",
      "Trial 41:\t{'hidden_size': 321, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.13462578033866046}\n",
      "0.9764999747276306\n",
      "Trial 42:\t{'hidden_size': 264, 'n_layers': 2, 'activation': <function relu at 0x7f1ffe869268>, 'lr': 0.009493107893987409, 'dropout': 0.13462578033866046}\n",
      "0.9729999899864197\n",
      "Trial 43:\t{'hidden_size': 321, 'n_layers': 3, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.13462578033866046}\n",
      "0.9769999980926514\n",
      "Trial 44:\t{'hidden_size': 407, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008283454143537825, 'dropout': 0.23873876732826038}\n",
      "0.9790999889373779\n",
      "Trial 45:\t{'hidden_size': 160, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008283454143537825, 'dropout': 0.13462578033866046}\n",
      "0.9749000072479248\n",
      "Trial 46:\t{'hidden_size': 264, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.13462578033866046}\n",
      "0.9751999974250793\n",
      "Trial 47:\t{'hidden_size': 264, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.13462578033866046}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9739999771118164\n",
      "Trial 48:\t{'hidden_size': 407, 'n_layers': 3, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.13462578033866046}\n",
      "0.9775999784469604\n",
      "Trial 49:\t{'hidden_size': 448, 'n_layers': 3, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008367673307662504, 'dropout': 0.13462578033866046}\n",
      "0.9803000092506409\n",
      "Trial 50:\t{'hidden_size': 407, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008283454143537825, 'dropout': 0.2511163133899563}\n",
      "0.9768000245094299\n",
      "Trial 51:\t{'hidden_size': 264, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008283454143537825, 'dropout': 0.13462578033866046}\n",
      "0.9776999950408936\n",
      "Trial 52:\t{'hidden_size': 407, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.009493107893987409, 'dropout': 0.13462578033866046}\n",
      "0.9797000288963318\n",
      "Trial 53:\t{'hidden_size': 407, 'n_layers': 2, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.007599994813545633, 'dropout': 0.13462578033866046}\n",
      "0.9768000245094299\n",
      "Trial 54:\t{'hidden_size': 448, 'n_layers': 6, 'activation': <function sigmoid at 0x7f1ffe86b950>, 'lr': 0.008283454143537825, 'dropout': 0.23873876732826038}\n"
     ]
    }
   ],
   "source": [
    "study = sherpa.Study(parameters=parameters,\n",
    "                     algorithm=algorithm,\n",
    "                     lower_is_better=False)\n",
    "for trial in study:\n",
    "    print(\"Trial {}:\\t{}\".format(trial.id, trial.parameters))\n",
    "    mlp=MLP(x_train.shape[1],10,\n",
    "            trial.parameters['n_layers'],\n",
    "            trial.parameters['hidden_size'],\n",
    "            trial.parameters['activation'],\n",
    "            trial.parameters['dropout'])\n",
    "    mlp.train()\n",
    "    optimizer=optim.Adam(mlp.parameters(), lr=trial.parameters['lr'])\n",
    "    for i in range(epochs):\n",
    "        for x_batch, y_batch in train_data:\n",
    "            optimizer.zero_grad()\n",
    "            out=mlp(x_batch)\n",
    "            loss=criterion(out,y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    mlp.eval()        \n",
    "    val_acc=(mlp(x_test_tensor).argmax(dim=1)==y_test_tensor).type(torch.float32).mean().item()\n",
    "    print(val_acc)\n",
    "    study.add_observation(trial=trial,\n",
    "                      iteration=epochs,\n",
    "                      objective=val_acc)\n",
    "    study.finalize(trial=trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Trial-ID': 14, 'Iteration': 15, 'activation': <function relu at 0x7f1ffe869268>, 'hidden_size': 74, 'n_layers': 3, 'Objective': 0.9758999943733215}\n"
     ]
    }
   ],
   "source": [
    "print(study.get_best_result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
