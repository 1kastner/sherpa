{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /Users/larshertel/python3env/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning:Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SHERPA is a Python library for hyperparameter tuning of machine learning models.\n",
    "Copyright (C) 2018  Lars Hertel, Peter Sadowski, and Julian Collado.\n",
    "\n",
    "This file is part of SHERPA.\n",
    "\n",
    "SHERPA is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "SHERPA is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with SHERPA.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "import sherpa\n",
    "import sherpa.algorithms.bayesian_optimization as bayesian_optimization\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sherpa Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sherpa.core:\n",
      "-------------------------------------------------------\n",
      "SHERPA Dashboard running. Access via\n",
      "http://169.234.54.206:8880 if on a cluster or\n",
      "http://localhost:8880 if running locally.\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "parameters = [sherpa.Continuous('learning_rate', [1e-4, 1e-2], 'log'),\n",
    "              sherpa.Discrete('num_units', [32, 128]),\n",
    "              sherpa.Choice('activation', ['relu', 'tanh', 'sigmoid'])]\n",
    "algorithm = alg = sherpa.algorithms.SuccessiveHalving(r=1, R=9, eta=3, s=0, max_finished_configs=5)\n",
    "study = sherpa.Study(parameters=parameters,\n",
    "                     algorithm=algorithm,\n",
    "                     lower_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Trial 1 for resource 1\n",
      "Loading model from:  \n",
      "Storing model as:  1\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.2554 - acc: 0.9257\n",
      "10000/10000 [==============================] - 0s 30us/step\n",
      "Training Trial 2 for resource 1\n",
      "Loading model from:  \n",
      "Storing model as:  2\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.2244 - acc: 0.9328\n",
      "10000/10000 [==============================] - 0s 30us/step\n",
      "Training Trial 3 for resource 1\n",
      "Loading model from:  \n",
      "Storing model as:  3\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.3908 - acc: 0.8944\n",
      "10000/10000 [==============================] - 0s 31us/step\n",
      "Training Trial 4 for resource 3\n",
      "Loading model from:  1\n",
      "Storing model as:  4\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.2109 - acc: 0.9397\n",
      "10000/10000 [==============================] - 0s 17us/step\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.1619 - acc: 0.9537\n",
      "10000/10000 [==============================] - 0s 18us/step\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.1314 - acc: 0.9620\n",
      "10000/10000 [==============================] - 0s 17us/step\n",
      "Training Trial 5 for resource 1\n",
      "Loading model from:  \n",
      "Storing model as:  5\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.4587 - acc: 0.8772\n",
      "10000/10000 [==============================] - 0s 32us/step\n",
      "Training Trial 6 for resource 1\n",
      "Loading model from:  \n",
      "Storing model as:  6\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2641 - acc: 0.9225\n",
      "10000/10000 [==============================] - 0s 31us/step\n",
      "Training Trial 7 for resource 1\n",
      "Loading model from:  \n",
      "Storing model as:  7\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.7980 - acc: 0.8064\n",
      "10000/10000 [==============================] - 0s 33us/step\n",
      "Training Trial 8 for resource 3\n",
      "Loading model from:  6\n",
      "Storing model as:  8\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3695 - acc: 0.9022\n",
      "10000/10000 [==============================] - 0s 18us/step\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.3002 - acc: 0.9165\n",
      "10000/10000 [==============================] - 0s 16us/step\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.2649 - acc: 0.9261\n",
      "10000/10000 [==============================] - 0s 15us/step\n",
      "Training Trial 9 for resource 1\n",
      "Loading model from:  \n",
      "Storing model as:  9\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.5150 - acc: 0.8764\n",
      "10000/10000 [==============================] - 0s 36us/step\n",
      "Training Trial 10 for resource 1\n",
      "Loading model from:  \n",
      "Storing model as:  10\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.5681 - acc: 0.8503\n",
      "10000/10000 [==============================] - 0s 34us/step\n",
      "Training Trial 11 for resource 1\n",
      "Loading model from:  \n",
      "Storing model as:  11\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.5152 - acc: 0.8751\n",
      "10000/10000 [==============================] - 0s 35us/step\n",
      "Training Trial 12 for resource 3\n",
      "Loading model from:  2\n",
      "Storing model as:  12\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.2380 - acc: 0.9329\n",
      "10000/10000 [==============================] - 0s 17us/step\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.1851 - acc: 0.9473\n",
      "10000/10000 [==============================] - 0s 18us/step\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.1530 - acc: 0.9560\n",
      "10000/10000 [==============================] - 0s 17us/step\n",
      "Training Trial 13 for resource 9\n",
      "Loading model from:  4\n",
      "Storing model as:  13\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.2569 - acc: 0.9245\n",
      "10000/10000 [==============================] - 0s 38us/step\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.1181 - acc: 0.9644\n",
      "10000/10000 [==============================] - 0s 24us/step\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0808 - acc: 0.9753\n",
      "10000/10000 [==============================] - 0s 20us/step\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0642 - acc: 0.9801\n",
      "10000/10000 [==============================] - 0s 26us/step\n",
      "Epoch 9/9\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0480 - acc: 0.9849\n",
      "10000/10000 [==============================] - 0s 31us/step\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0416 - acc: 0.9864\n",
      "10000/10000 [==============================] - 0s 22us/step\n",
      "Epoch 11/11\n",
      "14752/60000 [======>.......................] - ETA: 2s - loss: 0.0301 - acc: 0.9896"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-55b1615f729c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         study.add_observation(trial=trial, iteration=i,\n",
      "\u001b[0;32m~/python3env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/python3env/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5221\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_controller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5222\u001b[0m     context.context().context_switches.push(\n\u001b[0;32m-> 5223\u001b[0;31m         default.building_function, default.as_default)\n\u001b[0m\u001b[1;32m   5224\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5225\u001b[0m       with super(_DefaultGraphStack, self).get_controller(\n",
      "\u001b[0;32m~/python3env/lib/python3.6/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mpush\u001b[0;34m(self, is_building_function, enter_context_fn)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     self.stack.append(\n\u001b[0;32m--> 136\u001b[0;31m         ContextSwitch(is_building_function, enter_context_fn))\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "model_dict = {}\n",
    "\n",
    "for trial in study:\n",
    "    initial_epoch = {1: 0, 3: 1, 9: 4}[trial.parameters['resource']]\n",
    "    epochs = trial.parameters['resource'] + initial_epoch\n",
    "    \n",
    "    print(f\"Training Trial {trial.id} for resource {trial.parameters['resource']}\")\n",
    "    print(f\"Loading model from: \", trial.parameters['load_from'])\n",
    "    print(f\"Storing model as: \", trial.parameters['save_to'])\n",
    "    \n",
    "    \n",
    "    if trial.parameters['load_from'] not in model_dict:\n",
    "        lr = trial.parameters['learning_rate']\n",
    "        num_units = trial.parameters['num_units']\n",
    "        act = trial.parameters['activation']\n",
    "\n",
    "        # Create model\n",
    "        model = Sequential([Flatten(input_shape=(28, 28)),\n",
    "                            Dense(num_units, activation=act),\n",
    "                            Dense(10, activation='softmax')])\n",
    "        optimizer = Adam(lr=lr)\n",
    "        model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "        model_dict[trial.parameters['save_to']] = model\n",
    "        \n",
    "\n",
    "    # Train model\n",
    "    for i in range(initial_epoch, epochs):\n",
    "        model.fit(x_train, y_train, initial_epoch=i, epochs=i+1)\n",
    "        loss, accuracy = model.evaluate(x_test, y_test)\n",
    "        study.add_observation(trial=trial, iteration=i,\n",
    "                              objective=accuracy,\n",
    "                              context={'loss': loss})\n",
    "    study.finalize(trial=trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
